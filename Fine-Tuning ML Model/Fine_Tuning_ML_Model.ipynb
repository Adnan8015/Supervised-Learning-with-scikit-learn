{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How good is ML Model?"
      ],
      "metadata": {
        "id": "DgrgQRwaNJac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class Imbalance**"
      ],
      "metadata": {
        "id": "jX3VbcYFNSQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Consider a model for predicting whether a bank transaction is fraudulent, where only 1% of transactions are actually fraudulent.\n",
        "We could build a model that classifies every transaction as legitimate; this model would have an accuracy of 99%!\n",
        "\n",
        "However, it does a terrible job of actually predicting fraud, so it fails at its original purpose.\n",
        "The situation where one class is more frequent is called class imbalance.\n",
        "Here, the class of legitimate transactions contains way more instances than the class of fraudulent transactions.\n",
        "This is a common situation in practice and requires a different approach to assessing the model's performance.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "__1ExexaNNzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbrHp634NDbl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "1. True Positives (TP) – Correctly Detecting Fraud\n",
        "------------------------------------------------------\n",
        "Meaning: The model correctly predicts a transaction as fraud when it actually is fraud.\n",
        "\n",
        "\n",
        "2. True Negatives (TN) – Correctly Detecting Legitimate Transactions\n",
        "-----------------------------------------------------------------------\n",
        "Meaning: The model correctly predicts a transaction as legitimate when it actually is legitimate.\n",
        "\n",
        "\n",
        "3. False Negatives (FN) – Missing a Fraudulent Transaction\n",
        "-------------------------------------------------------------\n",
        "Meaning: The model predicts a transaction is legitimate when it is actually fraud (missed fraud).\n",
        "\n",
        "\n",
        "4. False Positives (FP) – Wrongly Flagging a Legitimate Transaction as Fraud\n",
        "--------------------------------------------------------------------------------\n",
        "Meaning: The model predicts a transaction is fraud when it is actually legitimate.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision**"
      ],
      "metadata": {
        "id": "ne9FKbLonAwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Precision = True Positives (TP) / (True Positives + False Positives)\n",
        "\n",
        "Or in simple words:\n",
        "--->  Precision tells us how many of the transactions labeled as \"fraud\" are actually fraud.\n",
        "\n",
        "\n",
        "\n",
        "Model predicts 10 transactions as fraud:\n",
        "\n",
        "7 are actually fraud (True Positives - TP)\n",
        "3 are legitimate but wrongly flagged as fraud (False Positives - FP)\n",
        "Precision is calculated as:\n",
        "Precision = 7/10 = 70%\n",
        "\n",
        "\n",
        "\n",
        "Why is Precision Important?\n",
        "---------------------------------\n",
        "A high precision means model rarely makes mistakes when predicting fraud.\n",
        "\n",
        "\n",
        "\n",
        "Example of High Precision (95%) → If model says a transaction is fraud, it's almost always correct.\n",
        "Example of Low Precision (50%) →  model often falsely flags normal transactions as fraud, frustrating customers.\n",
        "\n",
        "\n",
        "\n",
        "Connection to False Positives (FP)\n",
        "--------------------------------------\n",
        "High Precision = Fewer False Positives (Less inconvenience for customers)\n",
        "Low Precision = More False Positives (Good transactions get blocked too often)\n",
        "\n",
        "\n",
        "\n",
        "Real-Life Example of Precision Trade-Off\n",
        "If a bank wants high precision, it might only flag transactions when it’s really sure they’re fraud. This means:\n",
        "--->   Fewer false alarms (False Positives go down)\n",
        "--->   But it might miss some actual fraud cases (False Negatives go up)\n",
        "\n",
        "So, precision is useful when false positives are costly, like in fraud detection or medical diagnosis (where don’t want to wrongly tell someone they have a disease).\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2865mIzcnD0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recall**"
      ],
      "metadata": {
        "id": "DhwDrT74pbPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "Recall = True Positives (TP) / (True Positives + False Negatives)\n",
        "\n",
        "\n",
        "----> Recall tells us how many actual fraud cases our model correctly identified.\n",
        "\n",
        "\n",
        "\n",
        "A bank's fraud detection system is analyzing 100 transactions, and 20 of them are actually fraud.\n",
        "\n",
        "The model predicts some of them as fraud:\n",
        "\n",
        "15 fraud transactions are correctly detected  (True Positives - TP)\n",
        "5 fraud transactions are missed  (False Negatives - FN)\n",
        "\n",
        "Recall is calculated as:\n",
        "Recall = 15/20 = 75%\n",
        "\n",
        "\n",
        "Why is Recall Important?\n",
        "-----------------------------\n",
        "A high recall means the model detects most actual fraud cases.\n",
        "\n",
        "Example of High Recall (95%) → The model catches almost every fraud case.\n",
        "Example of Low Recall (50%) → The model misses many fraud cases, letting criminals go undetected.\n",
        "\n",
        "\n",
        "\n",
        "Connection to False Negatives (FN)\n",
        "-----------------------------------------\n",
        "High Recall = Fewer Missed Fraud Cases (Low FN)\n",
        "Low Recall = More Missed Fraud Cases (High FN)\n",
        "\n",
        "\n",
        "Real-Life Example of Recall Trade-Off\n",
        "--------------------------------------------\n",
        "If a bank wants high recall, it might flag any slightly suspicious transaction as fraud, which means:\n",
        " Almost all fraud cases get caught (Low FN)\n",
        " But many real customers may have their transactions wrongly blocked (High FP)\n",
        "\n",
        "So, recall is useful when missing fraud cases is costly, like in fraud detection or disease diagnosis (where don’t want to miss a serious illness).\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HB3mbUxcpdrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1 Score**"
      ],
      "metadata": {
        "id": "70wmOxS5rQJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The F1 Score is a single number that combines precision and recall into one metric. It helps us balance both precision and recall,\n",
        "especially when we can’t decide which is more important.\n",
        "\n",
        "F1 Score = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "​\n",
        "\n",
        "Why Use the F1 Score?\n",
        "---------------------------\n",
        "If precision is high but recall is low, the model is too strict and misses many fraud cases.\n",
        "If recall is high but precision is low, the model flags too many false fraud cases.\n",
        "F1-score finds a balance between these two\n",
        "\n",
        "\n",
        "\n",
        "In a fraud detection system:\n",
        "\n",
        "Model 1 (High Precision, Low Recall)\n",
        "\n",
        "Precision = 90% (few false fraud alarms)\n",
        "Recall = 50% (misses half the fraud case)\n",
        "F1-score = 2 × (0.90 × 0.50) / (0.90 + 0.50) = 0.64 (or 64%)\n",
        "\n",
        "\n",
        "\n",
        "Model 2 (High Recall, Low Precision)\n",
        "\n",
        "Precision = 60% (more false fraud alarms)\n",
        "Recall = 90% (catches most fraud cases)\n",
        "F1-score = 2 × (0.60 × 0.90) / (0.60 + 0.90) = 0.72 (or 72%)\n",
        "\n",
        "\n",
        "\n",
        "Model 3 (Balanced Precision & Recall)\n",
        "\n",
        "Precision = 80%\n",
        "Recall = 80%\n",
        "F1-score = 2 × (0.80 × 0.80) / (0.80 + 0.80) = 0.80 (or 80%)\n",
        "\n",
        "NOTE: The F1 score is highest when precision and recall are balanced.\n",
        "\n",
        "\n",
        "\n",
        "When to Use the F1 Score?\n",
        "--------------------------\n",
        "If both false positives and false negatives are costly (like fraud detection, medical diagnosis, spam filtering).\n",
        "When we want a balanced model instead of choosing only precision or recall.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-eb7zI4KrSj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import confusion_matrix and classification_report.\n",
        "Fit the model to the training data.\n",
        "Predict the labels of the test set, storing the results as y_pred.\n",
        "Compute and print the confusion matrix and classification report for the test labels versus the predicted labels\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import confusion matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "# Fit the model to the training data\n",
        "knn.fit(X_train , y_train)\n",
        "\n",
        "# Predict the labels of the test data: y_pred\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "lc4GhHePtqME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression and the ROC curve"
      ],
      "metadata": {
        "id": "OEqDW93ikZBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC Curve**"
      ],
      "metadata": {
        "id": "1IN5lhbNmUDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "What is the ROC Curve?\n",
        "-----------------------------\n",
        "The ROC (Receiver Operating Characteristic) curve helps us see how well a classification model distinguishes between two classes (e.g., fraud vs. not fraud)\n",
        "at different thresholds.\n",
        "\n",
        "It shows the trade-off between the True Positive Rate (TPR) and False Positive Rate (FPR) when we change the classification threshold.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Understanding Thresholds in Classification\n",
        "----------------------------------------------\n",
        "Most classification models predict probabilities instead of directly predicting labels (0 or 1).\n",
        "\n",
        "We convert probabilities into class labels using a threshold (default is usually 0.5).\n",
        "If probability > threshold → Predict 1 (fraud).\n",
        "If probability <= threshold → Predict 0 (not fraud).\n",
        "The ROC curve shows what happens when we change this threshold.\n",
        "\n",
        "\n",
        "\n",
        "How Threshold Affects the Model\n",
        "-----------------------------------\n",
        "1)  Threshold = 0 (Model predicts all 1s)\n",
        "\n",
        "The model predicts 1 (fraud) for every case, even non-fraud cases.\n",
        "True Positive Rate (TPR) = 100% (catches all fraud cases).\n",
        "False Positive Rate (FPR) = 100% (marks many non-fraud cases as fraud).\n",
        "Bad for real-world use (too many false alarms).\n",
        "\n",
        "\n",
        "\n",
        "2)  Threshold = 1 (Model predicts all 0s)\n",
        "\n",
        "The model predicts 0 (not fraud) for every case.\n",
        "True Positive Rate = 0% (misses all fraud cases).\n",
        "False Positive Rate = 0% (correctly classifies all non-fraud cases).\n",
        "Useless model because it never catches fraud!\n",
        "\n",
        "\n",
        "\n",
        "3) Varying the Threshold\n",
        "\n",
        "As we adjust the threshold, the trade-off between catching fraud and avoiding false alarms changes.\n",
        "Some thresholds will be better than others, depending on how much risk we can tolerate.\n",
        "\n",
        "What the ROC Curve Shows\n",
        "The X-axis = False Positive Rate (FPR) (bad predictions).\n",
        "The Y-axis = True Positive Rate (TPR) (good predictions).\n",
        "The curve shows different points for different thresholds.\n",
        " A good model’s ROC curve is above the diagonal line (random guessing).\n",
        " A perfect model would have a curve that goes straight up and across (TPR = 1, FPR = 0).\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eAiUy529kcUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC AUC**"
      ],
      "metadata": {
        "id": "jyB-AvRPm3-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "What is ROC AUC?\n",
        "------------------\n",
        "ROC AUC (Receiver Operating Characteristic - Area Under the Curve) is a metric that quantifies how well a classification model distinguishes between two classes (e.g., fraud vs. not fraud).\n",
        "\n",
        "AUC (Area Under Curve) measures how much the ROC curve is above the diagonal line (random guessing).\n",
        "\n",
        "Understanding AUC Scores\n",
        "-------------------------------\n",
        "AUC = 1.0 (Perfect Model)\n",
        "The model always correctly separates fraud and non-fraud cases.\n",
        "True Positive Rate = 100%, False Positive Rate = 0%.\n",
        "\n",
        "\n",
        "AUC = 0.5 (Random Guessing)\n",
        "The model is no better than flipping a coin (50% chance).\n",
        "The ROC curve is a straight diagonal line (TPR = FPR).\n",
        "\n",
        "\n",
        "AUC < 0.5 (Worse than Random)\n",
        "The model is making opposite predictions (classifying fraud as not fraud and vice versa).\n",
        "\n",
        "\n",
        "AUC = 0.67%\n",
        "The model is 67% good at distinguishing between fraud and non-fraud.\n",
        "This means it’s only 34%( (0.67−0.50) / (1.00−0.50) = 34% ) better than a random model (50%).\n",
        "It’s not great, but better than guessing.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "huLJOk07m6Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import LogisticRegression.\n",
        "Instantiate a logistic regression model, logreg.\n",
        "Fit the model to the training data.\n",
        "Predict the probabilities of each individual in the test set having a diabetes diagnosis, storing the array of positive probabilities as y_pred_probs.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate the model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train , y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_probs = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(y_pred_probs[:10])"
      ],
      "metadata": {
        "id": "UXpjNj2QoTG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import roc_curve.\n",
        "Calculate the ROC curve values, using y_test and y_pred_probs, and unpacking the results into fpr, tpr, and thresholds.\n",
        "Plot true positive rate against false positive rate.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import roc_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Generate ROC curve values: fpr, tpr, thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "# Plot tpr against fpr\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Diabetes Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dwm5-Ehboacj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Now you will compute the area under the ROC curve, along with the other classification metrics you have used previously.\n",
        "\n",
        "The confusion_matrix and classification_report functions have been preloaded for you, along with the logreg model you previously built, plus X_train, X_test, y_train, y_test.\n",
        "Also, the model's predicted test set labels are stored as y_pred, and probabilities of test set observations belonging to the positive class stored as y_pred_probs.\n",
        "\n",
        "A knn model has also been created and the performance metrics printed in the console, so you can compare the roc_auc_score, confusion_matrix, and classification_report between the two models.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import roc_auc_score.\n",
        "Calculate and print the ROC AUC score, passing the test labels and the predicted positive class probabilities.\n",
        "Calculate and print the confusion matrix.\n",
        "Call classification_report().\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import roc_auc_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Calculate roc_auc_score\n",
        "print(roc_auc_score(y_test, y_pred_probs))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Calculate the classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 0.8002483443708608\n",
        "#     [[121  30]\n",
        "#      [ 30  50]]\n",
        "#                   precision    recall  f1-score   support\n",
        "\n",
        "#                0       0.80      0.80      0.80       151\n",
        "#                1       0.62      0.62      0.62        80\n",
        "\n",
        "#         accuracy                           0.74       231\n",
        "#        macro avg       0.71      0.71      0.71       231\n",
        "#     weighted avg       0.74      0.74      0.74       231\n",
        "\n",
        "# logistic regression performs better than the KNN model across all the metrics you calculated.\n",
        "# A ROC AUC score of 0.8002 means this model is 60% better than a chance model at correctly predicting labels!\n"
      ],
      "metadata": {
        "id": "qyr8tIVno8gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Mdc9A0rEC-ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import GridSearchCV.\n",
        "Set up a parameter grid for \"alpha\", using np.linspace() to create 20 evenly spaced values ranging from 0.00001 to 1.\n",
        "Call GridSearchCV(), passing lasso, the parameter grid, and setting cv equal to kf.\n",
        "Fit the grid search object to the training data to perform a cross-validated grid search.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set up the parameter grid\n",
        "param_grid = {\"alpha\": np.linspace(.00001, 1, 20)}\n",
        "\n",
        "# Instantiate lasso_cv\n",
        "lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n",
        "\n",
        "# Fit to the training data\n",
        "lasso_cv.fit(X_train , y_train)\n",
        "print(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n",
        "print(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))"
      ],
      "metadata": {
        "id": "m3DpVxUNDs_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create params, adding \"l1\" and \"l2\" as penalty values, setting C to a range of 50 float values between 0.1 and 1.0, and class_weight to either \"balanced\" or a dictionary containing 0:0.8, 1:0.2.\n",
        "Create the Randomized Search CV object, passing the model and the parameters, and setting cv equal to kf.\n",
        "Fit logreg_cv to the training data.\n",
        "Print the model's best parameters and accuracy score\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Create the parameter space\n",
        "params = {\"penalty\": [\"l1\", \"l2\"],\n",
        "         \"tol\": np.linspace(0.0001, 1.0, 50),\n",
        "         \"C\": np.linspace(.1, 1, 50),\n",
        "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n",
        "\n",
        "# Instantiate the RandomizedSearchCV object\n",
        "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
        "\n",
        "# Fit the data to the model\n",
        "logreg_cv.fit(X_train, y_train)\n",
        "\n",
        "# Print the tuned parameters and score\n",
        "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
        "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))"
      ],
      "metadata": {
        "id": "4LeRn9dcGpGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}