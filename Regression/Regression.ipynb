{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Regression"
      ],
      "metadata": {
        "id": "f5z9JnEkXiau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFsSkqyEXJb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Create X, an array of the values from the sales_df DataFrame's \"radio\" column.\n",
        "Create y, an array of the values from the sales_df DataFrame's \"sales\" column.\n",
        "Reshape X into a two-dimensional NumPy array.\n",
        "Print the shape of X and y\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create X from the radio column's values\n",
        "X = sales_df[\"radio\"].values\n",
        "\n",
        "# Create y from the sales column's values\n",
        "y = sales_df[\"sales\"].values\n",
        "\n",
        "# Reshape X\n",
        "X = X.reshape(-1 , 1)\n",
        "\n",
        "# Check the shape of the features and targets\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import LinearRegression.\n",
        "Instantiate a linear regression model.\n",
        "Predict sales values using X, storing as predictions.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import LinearRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create the model\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "reg.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = reg.predict(X)\n",
        "\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "id": "KEtWXukWeI4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import matplotlib.pyplot as plt.\n",
        "Create a scatter plot visualizing y against X, with observations in blue.\n",
        "Draw a red line plot displaying the predictions against X.\n",
        "Display the plot.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scatter plot\n",
        "plt.scatter(X, y, color=\"blue\")\n",
        "\n",
        "# Create line plot\n",
        "plt.plot(X, predictions, color=\"red\")\n",
        "plt.xlabel(\"Radio Expenditure ($)\")\n",
        "plt.ylabel(\"Sales ($)\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxgzVWkRhKYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The basics of linear regression"
      ],
      "metadata": {
        "id": "dXThLfchz6O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create X, an array containing values of all features in sales_df, and y, containing all values from the \"sales\" column.\n",
        "Instantiate a linear regression model.\n",
        "Fit the model to the training data.\n",
        "Create y_pred, making predictions for sales using the test features\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create X and y arrays\n",
        "X = sales_df.drop(\"sales\", axis=1).values\n",
        "y = sales_df[\"sales\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate the model\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "reg.fit(X_train , y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))"
      ],
      "metadata": {
        "id": "BCiAPRvXz85X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import mean_squared_error.\n",
        "Calculate the model's R-squared score by passing the test feature values and the test target values to an appropriate method.\n",
        "Calculate the model's root mean squared error using y_test and y_pred.\n",
        "Print r_squared and rmse.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Compute R-squared\n",
        "r_squared = reg.score(X_test, y_test)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"R^2: {}\".format(r_squared))\n",
        "print(\"RMSE: {}\".format(rmse))"
      ],
      "metadata": {
        "id": "LX8WQHOS1f2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation"
      ],
      "metadata": {
        "id": "l7JbPQo-1q7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-validation motivation**"
      ],
      "metadata": {
        "id": "LTeys0kq2RU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "What is R-squared?\n",
        "\n",
        "--->  R-squared tells us how well our model's predictions match the actual values. A higher R-squared means our model is better at explaining the variation in the data.\n",
        "\n",
        "\n",
        "Problem with Single Train-Test Split\n",
        "--------------------------------------------\n",
        "When we split data into training and test sets, the test set is randomly chosen.\n",
        "Some test sets might have unusual patterns (e.g., outliers or rare cases).\n",
        "\n",
        "This can make the R-squared score unreliable, meaning it might not reflect how well the model works on new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "Imagine we're predicting house prices based on size.\n",
        "\n",
        "We randomly split the data:\n",
        "Train Set: Normal houses (apartments, single-family homes).\n",
        "Test Set: Mostly luxury villas (much more expensive).\n",
        "Our model, trained on normal houses, fails on the luxury houses, leading to a misleadingly low R-squared score.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "X131mQ2P2U4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import KFold and cross_val_score.\n",
        "Create kf by calling KFold(), setting the number of splits to six, shuffle to True, and setting a seed of 5.\n",
        "Perform cross-validation using reg on X and y, passing kf to cv.\n",
        "Print the cv_scores\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import the necessary modules\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "#Â Create a KFold object\n",
        "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
        "\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Compute 6-fold cross-validation scores\n",
        "cv_scores = cross_val_score(reg , X, y, cv=kf)\n",
        "\n",
        "# Print scores\n",
        "print(cv_scores)"
      ],
      "metadata": {
        "id": "2XVP9RId1sYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Calculate and print the mean of the results.\n",
        "Calculate and print the standard deviation of cv_results.\n",
        "Display the 95% confidence interval for your results using np.quantile()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Print the mean\n",
        "print(np.mean(cv_results))\n",
        "\n",
        "# Print the standard deviation\n",
        "print(np.std(cv_results))\n",
        "\n",
        "# Print the 95% confidence interval\n",
        "print(np.quantile(cv_results, [0.025, 0.975]))"
      ],
      "metadata": {
        "id": "P9GgFqIw6UZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized Regression"
      ],
      "metadata": {
        "id": "RohnAMcM6d3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression**"
      ],
      "metadata": {
        "id": "LDJYkTsM80ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "With ridge, we use the Ordinary Least Squares loss function plus the squared value of each coefficient, multiplied by a constant, alpha.\n",
        "\n",
        "So, when minimizing the loss function, models are penalized for coefficients with large positive or negative values.\n",
        "When using ridge, we need to choose the alpha value in order to fit and predict. Essentially, we can select the alpha for which our model performs best.\n",
        "Picking alpha for ridge is similar to picking k in KNN. Alpha in ridge is known as a hyperparameter, which is a variable used for selecting a model's parameters.\n",
        "\n",
        "lpha controls model complexity. When alpha equals zero, we are performing OLS, where large coefficients are not penalized and overfitting may occur.\n",
        "A high alpha means that large coefficients are significantly penalized, which can lead to underfitting.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kuhd_RaN83p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Regression**"
      ],
      "metadata": {
        "id": "g3SU845a9H-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "There is another type of regularized regression called lasso, where our loss function is the OLS loss function plus the absolute value of each coefficient multiplied by some constant, alpha.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8GVAFzJO9KdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso regression for feature selection**"
      ],
      "metadata": {
        "id": "BHhg-McV9T6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Lasso regression can actually be used to assess feature importance. This is because it tends to shrink the coefficients of less important features to zero.\n",
        "The features whose coefficients are not shrunk to zero are selected by the lasso algorithm.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cqunoWSo9VYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Regularized regression: Ridge\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import Ridge.\n",
        "Instantiate Ridge, setting alpha equal to alpha.\n",
        "Fit the model to the training data.\n",
        "Calculate the R^2 score for each iteration of ridge.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import Ridge\n",
        "from sklearn.linear_model import Ridge\n",
        "alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
        "ridge_scores = []\n",
        "for alpha in alphas:\n",
        "\n",
        "  # Create a Ridge regression model\n",
        "  ridge = Ridge(alpha = alpha)\n",
        "\n",
        "  # Fit the data\n",
        "  ridge.fit(X_train, y_train)\n",
        "\n",
        "  # Obtain R-squared\n",
        "  score = ridge.score(X_test, y_test)\n",
        "  ridge_scores.append(score)\n",
        "print(ridge_scores)"
      ],
      "metadata": {
        "id": "U5BvERUu6hTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Lasso regression for feature importance\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import Lasso from sklearn.linear_model.\n",
        "Instantiate a Lasso regressor with an alpha of 0.3.\n",
        "Fit the model to the data.\n",
        "Compute the model's coefficients, storing as lasso_coef\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import Lasso\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Instantiate a lasso regression model\n",
        "lasso = Lasso(alpha = 0.3)\n",
        "\n",
        "# Fit the model to the data\n",
        "lasso.fit(X , y)\n",
        "\n",
        "# Compute and print the coefficients\n",
        "lasso_coef = lasso.coef_\n",
        "print(lasso_coef)\n",
        "plt.bar(sales_columns, lasso_coef)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "###  [ 3.56256962 -0.00397035  0.00496385]"
      ],
      "metadata": {
        "id": "6puhyo09GMq0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}