{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "7G-hY7qaJM9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding dummy variables**"
      ],
      "metadata": {
        "id": "Lvh6vFk0OUNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "call pd-dot-get_dummies, passing the categorical column. As we only need to keep nine out of our ten binary features, we can set the drop_first argument to True.\n",
        "To bring these binary features back into our original DataFrame we can use pd-dot-concat, passing a list containing the music DataFrame and our dummies DataFrame,\n",
        "and setting axis equal to one. Lastly, we can remove the original genre column using df-dot-drop, passing the column, and setting axis equal to one\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "music_df = pd.read_csv('music.csv')\n",
        "music_dummies = pd.get_dummies(music_df[\"genre\"], drop_first=True)\n",
        "print(music_dummies.head())\n",
        "\n",
        "\n",
        "music_dummies = pd.concat([music_df, music_dummies], axis=1)\n",
        "music_dummies = music_dummies.drop(\"genre\", axis=1)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "If the DataFrame only has one categorical feature, we can pass the entire DataFrame, thus skipping the step of combining variables.\n",
        "If we don't specify a column, the new DataFrame's binary columns will have the original feature name prefixed,\n",
        "so they will start with genre-underscore. Notice the original genre column is automatically dropped. Once we have dummy variables, we can fit models as before.\n",
        "\n",
        "\n",
        "music_dummies = pd.get_dummies(music_df, drop_first=True)\n",
        "print(music_dummies.columns)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3ev5rPAnOWPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmlTpsXVJJtf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Use a relevant function, passing the entire music_df DataFrame, to create music_dummies, dropping the first binary column.\n",
        "Print the shape of music_dummies.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create music_dummies\n",
        "music_dummies = pd.get_dummies(music_df, drop_first = True)\n",
        "\n",
        "# Print the new DataFrame's shape\n",
        "print(\"Shape of music_dummies: {}\".format(music_dummies.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create X, containing all features in music_dummies, and y, consisting of the \"popularity\" column, respectively.\n",
        "Instantiate a ridge regression model, setting alpha equal to 0.2.\n",
        "Perform cross-validation on X and y using the ridge model, setting cv equal to kf, and using negative mean squared error as the scoring metric.\n",
        "Print the RMSE values by converting negative scores to positive and taking the square root.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create X and y\n",
        "X = music_dummies.drop(\"popularity\", axis=1).values\n",
        "y = music_dummies[\"popularity\"].values\n",
        "\n",
        "# Instantiate a ridge model\n",
        "ridge = Ridge(alpha=0.2)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(-scores)\n",
        "print(\"Average RMSE: {}\".format(np.mean(rmse)))\n",
        "print(\"Standard Deviation of the target array: {}\".format(np.std(y)))"
      ],
      "metadata": {
        "id": "2D8qtySVUlap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling missing Data"
      ],
      "metadata": {
        "id": "TeDdUMosfZGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping missing data**"
      ],
      "metadata": {
        "id": "tgxu5AZAfeFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "A common approach is to remove missing observations accounting for less than 5% of all data.\n",
        "To do this, we use pandas' dot-dropna method, passing a list of columns with less than 5% missing values to the subset argument.\n",
        "If there are missing values in our subset column, the entire row is removed. Rechecking the DataFrame, we see fewer missing values\n",
        "\n",
        "\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "\n",
        "music_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9nM25wXnfhAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputing Values**"
      ],
      "metadata": {
        "id": "Z83Gd2CrgBiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Another option is to impute missing data. This means making an educated guess as to what the missing values could be.\n",
        "We can impute the mean of all non-missing entries for a given feature. We can also use other values like the median.\n",
        "For categorical values we commonly impute the most frequent value.\n",
        "\n",
        "\n",
        "Note: we must split our data before imputing to avoid leaking test set information to our model, a concept known as data leakage\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FyFHOC76gD9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Dropping missing data\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Print the number of missing values for each column in the music_df dataset, sorted in ascending order.\n",
        "Remove values for all columns with 50 or fewer missing values.\n",
        "Convert music_df[\"genre\"] to values of 1 if the row contains \"Rock\", otherwise change the value to 0.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Print missing values for each column\n",
        "print(music_df.isna().sum().sort_values())\n",
        "\n",
        "# Remove values where less than 5% are missing\n",
        "music_df = music_df.dropna(subset=[\"genre\" , \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n",
        "\n",
        "# Convert genre to a binary feature\n",
        "music_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n",
        "\n",
        "print(music_df.isna().sum().sort_values())\n",
        "print(\"Shape of the `music_df`: {}\".format(music_df.shape))"
      ],
      "metadata": {
        "id": "IQYJ2NzEfcGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Pipeline for song genre prediction: I\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import SimpleImputer and Pipeline.\n",
        "Instantiate an imputer.\n",
        "Instantiate a KNN classifier with three neighbors.\n",
        "Create steps, a list of tuples containing the imputer variable you created, called \"imputer\", followed by the knn model you created, called \"knn\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import modules\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Instantiate an imputer\n",
        "imputer = SimpleImputer()\n",
        "\n",
        "# Instantiate a knn model\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "\n",
        "# Build steps for the pipeline\n",
        "steps = [(\"imputer\", imputer),\n",
        "         (\"knn\", knn)]"
      ],
      "metadata": {
        "id": "gPnDXZozn2bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Pipeline for song genre prediction: II\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Create a pipeline using the steps you previously defined.\n",
        "Fit the pipeline to the training data.\n",
        "Make predictions on the test set.\n",
        "Calculate and print the confusion matrix.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "steps = [(\"imputer\", imp_mean),\n",
        "        (\"knn\", knn)]\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train , y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "8HIjjLEyu9bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Centering and Scaling"
      ],
      "metadata": {
        "id": "IAGHYAi0vkPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why scale our data?**"
      ],
      "metadata": {
        "id": "x9Fd1HJ4vqoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Many machine learning models use some form of distance to inform them, so if we have features on far larger scales, they can disproportionately influence our model.\n",
        "\n",
        "For example, KNN uses distance explicitly when making predictions. For this reason, we actually want features to be on a similar scale.\n",
        "To achieve this, we can normalize or standardize our data, often referred to as scaling and centering.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xnNihOOEvm57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to scale our data**"
      ],
      "metadata": {
        "id": "b8vk3-GcxsuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "There are several ways to scale our data: given any column, we can subtract the mean and divide by the variance so that all features are centered around zero and have a variance of one.\n",
        "This is called standardization.\n",
        "\n",
        "We can also subtract the minimum and divide by the range of the data so the normalized dataset has minimum zero and maximum one.\n",
        "\n",
        "Or, we can center our data so that it ranges from -1 to 1 instead.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QSWCM4dcxvE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CV and scaling in a pipeline**"
      ],
      "metadata": {
        "id": "5HOZmmH-yPsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "We first build our pipeline. We then specify our hyperparameter space by creating a dictionary: the keys are the pipeline step name followed by a double underscore, followed by the hyperparameter name.\n",
        "The corresponding value is a list or an array of the values to try for that particular hyperparameter.\n",
        "\n",
        "In this case, we are tuning n_neighbors in the KNN model. Next we split our data into training and test sets.\n",
        "We then perform a grid search over our parameters by instantiating the GridSearchCV object, passing our pipeline and setting the param_grid argument equal to parameters.\n",
        "We then fit it to our training data. Lastly, we make predictions using our test set.\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "steps = [('scaler', StandardScaler()),\n",
        "         ('knn', KNeighborsClassifier())]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "parameters = {\"knn__n_neighbors\": np.arange(1, 50)}\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=21)\n",
        "\n",
        "\n",
        "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
        "cv.fit(X_train, y_train)\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PFut-SIIy4Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Centering and scaling for regression\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Now you have seen the benefits of scaling your data, you will use a pipeline to preprocess the music_df features and build a lasso regression model to predict a song's loudness.\n",
        "\n",
        "X_train, X_test, y_train, and y_test have been created from the music_df dataset, where the target is \"loudness\" and the features are all other columns in the dataset.\n",
        "Lasso and Pipeline have also been imported for you.\n",
        "\n",
        "Note that \"genre\" has been converted to a binary feature where 1 indicates a rock song, and 0 represents other genres\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Import StandardScaler.\n",
        "Create the steps for the pipeline object, a StandardScaler object called \"scaler\", and a lasso model called \"lasso\" with alpha set to 0.5.\n",
        "Instantiate a pipeline with steps to scale and build a lasso regression model.\n",
        "Calculate the R-squared value on the test data.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create pipeline steps\n",
        "steps = [(\"scaler\", StandardScaler()),\n",
        "         (\"lasso\", Lasso(alpha=0.5))]\n",
        "\n",
        "# Instantiate the pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print R-squared\n",
        "print(pipeline.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "gke0NbAsztmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Build the steps for the pipeline: a StandardScaler() object named \"scaler\", and a logistic regression model named \"logreg\".\n",
        "Create the parameters, searching 20 equally spaced float values ranging from 0.001 to 1.0 for the logistic regression model's C hyperparameter within the pipeline.\n",
        "Instantiate the grid search object.\n",
        "Fit the grid search object to the training data.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Build the steps\n",
        "steps = [(\"scaler\", StandardScaler()),\n",
        "         (\"logreg\", LogisticRegression())]\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# Create the parameter space\n",
        "parameters = {\"logreg__C\": np.linspace(0.001, 1, 20)}\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=21)\n",
        "\n",
        "# Instantiate the grid search object\n",
        "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
        "\n",
        "# Fit to the training data\n",
        "cv.fit(X_train, y_train)\n",
        "print(cv.best_score_, \"\\n\", cv.best_params_)"
      ],
      "metadata": {
        "id": "q2Q7VA0b0Wn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating multiple Models"
      ],
      "metadata": {
        "id": "IBnDyw6M4IqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Visualizing regression model performance\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Write a for loop using model as the iterator, and model.values() as the iterable.\n",
        "Perform cross-validation on the training features and the training target array using the model, setting cv equal to the KFold object.\n",
        "Append the model's cross-validation scores to the results list.\n",
        "Create a box plot displaying the results, with the x-axis labels as the names of the models.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "models = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(alpha=0.1), \"Lasso\": Lasso(alpha=0.1)}\n",
        "results = []\n",
        "\n",
        "# Loop through the models' values\n",
        "for model in models.values():\n",
        "  kf = KFold(n_splits=6, random_state=42, shuffle=True)\n",
        "\n",
        "  # Perform cross-validation\n",
        "  cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n",
        "\n",
        "  # Append the results\n",
        "  results.append(cv_scores)\n",
        "\n",
        "# Create a box plot of the results\n",
        "plt.boxplot(results, labels=models.keys())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d4pNanSG4P0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Import mean_squared_error.\n",
        "Fit the model to the scaled training features and the training labels.\n",
        "Make predictions using the scaled test features.\n",
        "Calculate RMSE by passing the test set labels and the predicted labels.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "for name, model in models.items():\n",
        "\n",
        "  # Fit the model to the training data\n",
        "  model.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "  # Calculate the test_rmse\n",
        "  test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "  print(\"{} Test Set RMSE: {}\".format(name, test_rmse))"
      ],
      "metadata": {
        "id": "CHfs0rmnCL8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create a dictionary of \"Logistic Regression\", \"KNN\", and \"Decision Tree Classifier\", setting the dictionary's values to a call of each model.\n",
        "Loop through the values in models.\n",
        "Instantiate a KFold object to perform 6 splits, setting shuffle to True and random_state to 12.\n",
        "Perform cross-validation using the model, the scaled training features, the target training set, and setting cv equal to kf\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create models dictionary\n",
        "models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\":  DecisionTreeClassifier()}\n",
        "results = []\n",
        "\n",
        "# Loop through the models' values\n",
        "for model in models.values():\n",
        "\n",
        "  # Instantiate a KFold object\n",
        "  kf = KFold(n_splits=6, random_state=12, shuffle=True)\n",
        "\n",
        "  # Perform cross-validation\n",
        "  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
        "  results.append(cv_results)\n",
        "plt.boxplot(results, labels=models.keys())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k6ssCREVComH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Pipeline for predicting song popularity\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "For the final exercise, you will build a pipeline to impute missing values, scale features, and perform hyperparameter tuning of a logistic regression model.\n",
        "The aim is to find the best parameters and accuracy when predicting song genre!\n",
        "\n",
        "All the models and objects required to build the pipeline have been preloaded for you\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Create the steps for the pipeline by calling a simple imputer, a standard scaler, and a logistic regression model.\n",
        "Create a pipeline object, and pass the steps variable.\n",
        "Instantiate a grid search object to perform cross-validation using the pipeline and the parameters.\n",
        "Print the best parameters and compute and print the test set accuracy score for the grid search object.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create steps\n",
        "steps = [(\"imp_mean\", SimpleImputer()),\n",
        "         (\"scaler\", StandardScaler()),\n",
        "         (\"logreg\", LogisticRegression())]\n",
        "\n",
        "# Set up pipeline\n",
        "pipeline = Pipeline(steps)\n",
        "params = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n",
        "         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "tuning = GridSearchCV(pipeline, param_grid=params)\n",
        "tuning.fit(X_train, y_train)\n",
        "y_pred = tuning.predict(X_test)\n",
        "\n",
        "# Compute and print performance\n",
        "print(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "SCV4joJfDhZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}